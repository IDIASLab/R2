##############################
MLPClassifier Full Results
##############################
   hidden_layer_sizes   alpha  learning_rate_init  max_iter  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            (64, 64)  0.0001               0.001        50  0.839351  0.782782     0.721541     0.808833  0.616200  0.872262
1            (64, 64)  0.0001               0.001       100  0.839351  0.782782     0.721541     0.808833  0.616200  0.872262
2            (64, 64)  0.0001               0.010        50  0.832544  0.771737     0.710804     0.796316  0.584662  0.872225
3            (64, 64)  0.0001               0.010       100  0.832544  0.771737     0.710804     0.796316  0.584662  0.872225
4            (64, 64)  0.0010               0.001        50  0.843019  0.783476     0.745020     0.797789  0.578299  0.893687
5            (64, 64)  0.0010               0.001       100  0.843019  0.783476     0.745020     0.797789  0.578299  0.893687
6            (64, 64)  0.0010               0.010        50  0.822014  0.733486     0.770720     0.726752  0.337826  0.946016
7            (64, 64)  0.0010               0.010       100  0.822014  0.733486     0.770720     0.726752  0.337826  0.946016
8            (64, 64)  0.0100               0.001        50  0.830518  0.773124     0.696482     0.807866  0.621673  0.854476
9            (64, 64)  0.0100               0.001       100  0.830518  0.773124     0.696482     0.807866  0.621673  0.854476
10           (64, 64)  0.0100               0.010        50  0.810857  0.715461     0.721995     0.714342  0.302046  0.937528
11           (64, 64)  0.0100               0.010       100  0.810857  0.715461     0.721995     0.714342  0.302046  0.937528
12         (128, 128)  0.0001               0.001        50  0.841675  0.785364     0.728577     0.809114  0.614832  0.876966
13         (128, 128)  0.0001               0.001       100  0.841675  0.785364     0.728577     0.809114  0.614832  0.876966
14         (128, 128)  0.0001               0.010        50  0.832817  0.771474     0.713166     0.794560  0.578847  0.874945
15         (128, 128)  0.0001               0.010       100  0.832817  0.771474     0.713166     0.794560  0.578847  0.874945
16         (128, 128)  0.0010               0.001        50  0.844872  0.787922     0.742325     0.805959  0.602107  0.887733
17         (128, 128)  0.0010               0.001       100  0.844872  0.787922     0.742325     0.805959  0.602107  0.887733
18         (128, 128)  0.0010               0.010        50  0.824407  0.737885     0.776700     0.730612  0.350756  0.945833
19         (128, 128)  0.0010               0.010       100  0.824407  0.737885     0.776700     0.730612  0.350756  0.945833
20         (128, 128)  0.0100               0.001        50  0.831241  0.771833     0.703131     0.801070  0.600670  0.863773
21         (128, 128)  0.0100               0.001       100  0.831241  0.771833     0.703131     0.801070  0.600670  0.863773
22         (128, 128)  0.0100               0.010        50  0.811988  0.712807     0.753012     0.707167  0.265102  0.953293
23         (128, 128)  0.0100               0.010       100  0.811988  0.712807     0.753012     0.707167  0.265102  0.953293
24         (256, 256)  0.0001               0.001        50  0.847033  0.793612     0.737385     0.817861  0.635835  0.878362
25         (256, 256)  0.0001               0.001       100  0.847033  0.793612     0.737385     0.817861  0.635835  0.878362
26         (256, 256)  0.0001               0.010        50  0.829836  0.770279     0.699173     0.800834  0.601355  0.861017
27         (256, 256)  0.0001               0.010       100  0.829836  0.770279     0.699173     0.800834  0.601355  0.861017
28         (256, 256)  0.0010               0.001        50  0.846873  0.791723     0.743064     0.811637  0.617500  0.885308
29         (256, 256)  0.0010               0.001       100  0.846873  0.791723     0.743064     0.811637  0.617500  0.885308
30         (256, 256)  0.0010               0.010        50  0.799507  0.731885     0.629808     0.778451  0.564617  0.821733
31         (256, 256)  0.0010               0.010       100  0.799507  0.731885     0.629808     0.778451  0.564617  0.821733
32         (256, 256)  0.0100               0.001        50  0.834437  0.776519     0.709436     0.805368  0.610522  0.865684
33         (256, 256)  0.0100               0.001       100  0.834437  0.776519     0.709436     0.805368  0.610522  0.865684
34         (256, 256)  0.0100               0.010        50  0.783948  0.711181     0.596617     0.763560  0.535678  0.805453
35         (256, 256)  0.0100               0.010       100  0.783948  0.711181     0.596617     0.763560  0.535678  0.805453

Best Parameters for MLPClassifier:
{'hidden_layer_sizes': (256, 256), 'alpha': 0.0001, 'learning_rate_init': 0.001, 'max_iter': 100, 'f1_score': 0.8470329748214823, 'accuracy': 0.7936120873078486, 'precision_0': 0.7373849571564582, 'precision_1': 0.8178614200171086, 'recall_0': 0.6358349866593692, 'recall_1': 0.8783624871380273}

Classification Report for Best MLPClassifier:
              precision    recall  f1-score   support

           0       0.74      0.64      0.68     14617
           1       0.82      0.88      0.85     27212

    accuracy                           0.79     41829
   macro avg       0.78      0.76      0.76     41829
weighted avg       0.79      0.79      0.79     41829



##############################
XGBClassifier Full Results
##############################
    n_estimators  max_depth  scale_pos_weight  learning_rate  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            100          5               0.5           0.01  0.667682  0.640058     0.490752     0.835912  0.796880  0.555821
1            100          5               0.5           0.10  0.758518  0.717206     0.569505     0.853259  0.781419  0.682714
2            100          5               0.5           0.20  0.773115  0.732315     0.586846     0.861692  0.790518  0.701051
3            100          5               1.0           0.01  0.803777  0.685505     0.865500     0.676467  0.118424  0.990115
4            100          5               1.0           0.10  0.824027  0.747520     0.724684     0.753803  0.447493  0.908680
5            100          5               1.0           0.20  0.829267  0.760788     0.720939     0.774033  0.514675  0.892988
6            100         10               0.5           0.01  0.732558  0.698678     0.545932     0.866740  0.818431  0.634352
7            100         10               0.5           0.10  0.806989  0.766287     0.631613     0.871960  0.794691  0.751029
8            100         10               0.5           0.20  0.820449  0.777236     0.654535     0.862462  0.767736  0.782339
9            100         10               1.0           0.01  0.821631  0.723780     0.859086     0.708418  0.250667  0.977914
10           100         10               1.0           0.10  0.845948  0.788137     0.749891     0.802665  0.590750  0.894164
11           100         10               1.0           0.20  0.845146  0.789452     0.738741     0.810256  0.614969  0.883177
12           100         15               0.5           0.01  0.776657  0.738411     0.591650     0.873508  0.811521  0.699140
13           100         15               0.5           0.10  0.832000  0.786177     0.679491     0.850957  0.734624  0.813869
14           100         15               0.5           0.20  0.838742  0.789476     0.701309     0.835925  0.692481  0.841577
15           100         15               1.0           0.01  0.833270  0.749074     0.838397     0.733856  0.349251  0.963839
16           100         15               1.0           0.10  0.848942  0.794162     0.749398     0.812261  0.617432  0.889093
17           100         15               1.0           0.20  0.845630  0.790719     0.737696     0.812890  0.622426  0.881119
18           200          5               0.5           0.01  0.687446  0.656458     0.505354     0.842198  0.797428  0.580736
19           200          5               0.5           0.10  0.773828  0.732745     0.587641     0.860860  0.788534  0.702778
20           200          5               0.5           0.20  0.788792  0.748261     0.606516     0.868354  0.796059  0.722586
21           200          5               1.0           0.01  0.810581  0.701116     0.847976     0.689611  0.176302  0.983022
22           200          5               1.0           0.10  0.831398  0.763274     0.728639     0.774605  0.513991  0.897178
23           200          5               1.0           0.20  0.837011  0.774654     0.731555     0.790431  0.560991  0.889424
24           200         10               0.5           0.01  0.757980  0.720146     0.570406     0.866468  0.806732  0.673637
25           200         10               0.5           0.10  0.823518  0.780918     0.659305     0.865137  0.771978  0.785720
26           200         10               0.5           0.20  0.830897  0.785675     0.676338     0.853583  0.741534  0.809386
27           200         10               1.0           0.01  0.831265  0.750628     0.789809     0.742451  0.390231  0.944216
28           200         10               1.0           0.10  0.849095  0.794186     0.750500     0.811744  0.615721  0.890049
29           200         10               1.0           0.20  0.845501  0.791054     0.735627     0.814599  0.627625  0.878840
30           200         15               0.5           0.01  0.786566  0.747496     0.603682     0.873754  0.807621  0.715199
31           200         15               0.5           0.10  0.841576  0.793756     0.705334     0.841097  0.703838  0.842055
32           200         15               0.5           0.20  0.843958  0.793947     0.717224     0.831745  0.677430  0.856534
33           200         15               1.0           0.01  0.840002  0.772957     0.764354     0.775548  0.506397  0.916140
34           200         15               1.0           0.10  0.850533  0.797055     0.750204     0.816449  0.628515  0.887586
35           200         15               1.0           0.20  0.847704  0.793803     0.741457     0.815880  0.629404  0.882111
36           300          5               0.5           0.01  0.706764  0.672715     0.520734     0.847181  0.796401  0.606277
37           300          5               0.5           0.10  0.781915  0.741352     0.597725     0.865964  0.794623  0.712737
38           300          5               0.5           0.20  0.799952  0.759760     0.621438     0.872768  0.799617  0.738351
39           300          5               1.0           0.01  0.815445  0.716512     0.788056     0.707262  0.258193  0.962700
40           300          5               1.0           0.10  0.836219  0.772024     0.734862     0.784986  0.543819  0.894605
41           300          5               1.0           0.20  0.840434  0.781085     0.734254     0.799171  0.585414  0.886190
42           300         10               0.5           0.01  0.774795  0.735303     0.589173     0.867620  0.801190  0.699912
43           300         10               0.5           0.10  0.831083  0.787659     0.674221     0.861253  0.759185  0.802955
44           300         10               0.5           0.20  0.838081  0.791939     0.693369     0.848739  0.725388  0.827686
45           300         10               1.0           0.01  0.834361  0.763752     0.752372     0.767043  0.482862  0.914633
46           300         10               1.0           0.10  0.851452  0.798178     0.752846     0.816868  0.628925  0.889093
47           300         10               1.0           0.20  0.846836  0.793038     0.738019     0.816541  0.632141  0.879465
48           300         15               0.5           0.01  0.796096  0.756270     0.616112     0.873393  0.802627  0.731369
49           300         15               0.5           0.10  0.844909  0.796242     0.716314     0.836824  0.690292  0.853153
50           300         15               0.5           0.20  0.845264  0.795262     0.720993     0.831414  0.675515  0.859584
51           300         15               1.0           0.01  0.842955  0.781491     0.752606     0.791597  0.558186  0.901441
52           300         15               1.0           0.10  0.851067  0.797915     0.750895     0.817465  0.631046  0.887550
53           300         15               1.0           0.20  0.849385  0.796314     0.744408     0.818368  0.635219  0.882846

Best Parameters for XGBClassifier:
{'n_estimators': 300.0, 'max_depth': 10.0, 'scale_pos_weight': 1.0, 'learning_rate': 0.1, 'f1_score': 0.8514516980468063, 'accuracy': 0.7981782973535108, 'precision_0': 0.7528457947752026, 'precision_1': 0.8168681207373895, 'recall_0': 0.6289252240541835, 'recall_1': 0.8890930471850654}

Classification Report for Best XGBClassifier:
              precision    recall  f1-score   support

           0       0.75      0.63      0.69     14617
           1       0.82      0.89      0.85     27212

    accuracy                           0.80     41829
   macro avg       0.78      0.76      0.77     41829
weighted avg       0.79      0.80      0.79     41829



##############################
LGBMClassifier Full Results
##############################
    n_estimators  max_depth  learning_rate  is_unbalance  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            100          5           0.01          True  0.772943  0.698104     0.574078     0.756724  0.527263  0.789872
1            100          5           0.01         False  0.802568  0.683138     0.856993     0.674825  0.111924  0.989968
2            100          5           0.10          True  0.764799  0.720648     0.575723     0.845520  0.762537  0.698148
3            100          5           0.10         False  0.823605  0.745320     0.729186     0.749525  0.431415  0.913935
4            100          5           0.20          True  0.778769  0.734945     0.593258     0.852028  0.768147  0.717110
5            100          5           0.20         False  0.829024  0.759856     0.722157     0.772155  0.508381  0.894936
6            100         10           0.01          True  0.781737  0.713596     0.593160     0.775184  0.574331  0.788402
7            100         10           0.01         False  0.810520  0.700471     0.857779     0.688674  0.171239  0.984749
8            100         10           0.10          True  0.777598  0.733893     0.591775     0.852082  0.768899  0.715089
9            100         10           0.10         False  0.828467  0.757465     0.725904     0.767248  0.491551  0.900301
10           100         10           0.20          True  0.790702  0.747926     0.609120     0.859751  0.777725  0.731920
11           100         10           0.20         False  0.833728  0.768629     0.727919     0.782861  0.539577  0.891665
12           100         15           0.01          True  0.782411  0.714433     0.594457     0.775727  0.575221  0.789211
13           100         15           0.01         False  0.811282  0.702144     0.857048     0.690082  0.177191  0.984125
14           100         15           0.10          True  0.777474  0.733797     0.591622     0.852149  0.769104  0.714832
15           100         15           0.10         False  0.827702  0.756676     0.722640     0.767326  0.492851  0.898390
16           100         15           0.20          True  0.788133  0.745272     0.605653     0.858703  0.776904  0.728282
17           100         15           0.20         False  0.834593  0.769777     0.730432     0.783507  0.540740  0.892805
18           200          5           0.01          True  0.738022  0.688757     0.541301     0.815638  0.716426  0.673894
19           200          5           0.01         False  0.810207  0.700017     0.853434     0.688474  0.170897  0.984235
20           200          5           0.10          True  0.780431  0.736905     0.595455     0.853726  0.770746  0.718727
21           200          5           0.10         False  0.830346  0.761027     0.728288     0.771477  0.504276  0.898942
22           200          5           0.20          True  0.796418  0.753807     0.617002     0.861843  0.779093  0.740225
23           200          5           0.20         False  0.836189  0.773698     0.728832     0.790214  0.561196  0.887844
24           200         10           0.01          True  0.756282  0.705993     0.562402     0.820759  0.714921  0.701198
25           200         10           0.01         False  0.816609  0.718497     0.793959     0.708636  0.262571  0.963399
26           200         10           0.10          True  0.789075  0.746516     0.606915     0.860173  0.779435  0.728833
27           200         10           0.10         False  0.835791  0.770757     0.736145     0.782574  0.536157  0.896773
28           200         10           0.20          True  0.800757  0.758517     0.623092     0.864296  0.781966  0.745921
29           200         10           0.20         False  0.840381  0.779890     0.738116     0.795464  0.573647  0.890673
30           200         15           0.01          True  0.756910  0.706472     0.563104     0.820527  0.713963  0.702447
31           200         15           0.01         False  0.817166  0.719907     0.792381     0.710155  0.268933  0.962149
32           200         15           0.10          True  0.789425  0.746755     0.607387     0.859828  0.778546  0.729678
33           200         15           0.10         False  0.834111  0.768677     0.730608     0.781784  0.535472  0.893944
34           200         15           0.20          True  0.801743  0.759569     0.624495     0.864767  0.782445  0.747281
35           200         15           0.20         False  0.840512  0.780105     0.738324     0.795699  0.574263  0.890673
36           300          5           0.01          True  0.735431  0.690574     0.541600     0.828642  0.745502  0.661069
37           300          5           0.01         False  0.814416  0.715006     0.780483     0.706507  0.256619  0.961230
38           300          5           0.10          True  0.787431  0.744627     0.604716     0.858724  0.777314  0.727069
39           300          5           0.10         False  0.835326  0.770207     0.734514     0.782431  0.536225  0.895892
40           300          5           0.20          True  0.804799  0.762724     0.628929     0.865739  0.782924  0.751874
41           300          5           0.20         False  0.842531  0.783954     0.739444     0.801140  0.589451  0.888432
42           300         10           0.01          True  0.755971  0.708862     0.563725     0.831262  0.738045  0.693187
43           300         10           0.01         False  0.819814  0.731454     0.752537     0.727433  0.344941  0.939071
44           300         10           0.10          True  0.794379  0.751966     0.614130     0.862164  0.780803  0.736477
45           300         10           0.10         False  0.840061  0.777953     0.742912     0.790408  0.557502  0.896369
46           300         10           0.20          True  0.809386  0.766956     0.635998     0.864928  0.778888  0.760547
47           300         10           0.20         False  0.843373  0.785412     0.740390     0.802964  0.594308  0.888064
48           300         15           0.01          True  0.757387  0.710082     0.565334     0.831204  0.737019  0.695612
49           300         15           0.01         False  0.819962  0.732124     0.750735     0.728508  0.349456  0.937675
50           300         15           0.10          True  0.795974  0.753807     0.616309     0.863554  0.782856  0.738204
51           300         15           0.10         False  0.839248  0.777140     0.739571     0.790630  0.559143  0.894238
52           300         15           0.20          True  0.807511  0.765211     0.633074     0.865219  0.780461  0.757019
53           300         15           0.20         False  0.843969  0.786416     0.741153     0.804167  0.597455  0.887917

Best Parameters for LGBMClassifier:
{'n_estimators': 300, 'max_depth': 15, 'learning_rate': 0.2, 'is_unbalance': False, 'f1_score': 0.8439694016556638, 'accuracy': 0.7864161227856272, 'precision_0': 0.7411525078502927, 'precision_1': 0.8041669440191707, 'recall_0': 0.5974550181295751, 'recall_1': 0.8879170953990886}

Classification Report for Best LGBMClassifier:
              precision    recall  f1-score   support

           0       0.74      0.60      0.66     14617
           1       0.80      0.89      0.84     27212

    accuracy                           0.79     41829
   macro avg       0.77      0.74      0.75     41829
weighted avg       0.78      0.79      0.78     41829



##############################
Ensemble Model Performance
##############################
Ensemble Accuracy: 0.8048
Ensemble Classification Report:
              precision    recall  f1-score   support

           0       0.76      0.64      0.70     14617
           1       0.82      0.89      0.86     27212

    accuracy                           0.80     41829
   macro avg       0.79      0.77      0.78     41829
weighted avg       0.80      0.80      0.80     41829

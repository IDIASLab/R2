##############################
MLPClassifier Full Results
##############################
   hidden_layer_sizes   alpha  learning_rate_init  max_iter  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            (64, 64)  0.0001               0.001        50  0.810472  0.814747     0.834818     0.794557  0.803450  0.827038
1            (64, 64)  0.0001               0.001       100  0.815052  0.818112     0.842275     0.794381  0.800910  0.836826
2            (64, 64)  0.0001               0.010        50  0.815358  0.808515     0.872941     0.757510  0.740262  0.882773
3            (64, 64)  0.0001               0.010       100  0.816489  0.814637     0.858017     0.776347  0.772015  0.861009
4            (64, 64)  0.0010               0.001        50  0.828102  0.829032     0.861421     0.798610  0.800699  0.859857
5            (64, 64)  0.0010               0.001       100  0.823735  0.827156     0.849380     0.805079  0.812341  0.843275
6            (64, 64)  0.0010               0.010        50  0.753975  0.754247     0.786765     0.724226  0.724809  0.786274
7            (64, 64)  0.0010               0.010       100  0.753975  0.754247     0.786765     0.724226  0.724809  0.786274
8            (64, 64)  0.0100               0.001        50  0.822697  0.829914     0.837720     0.821470  0.835415  0.823929
9            (64, 64)  0.0100               0.001       100  0.844541  0.850706     0.858449     0.842365  0.854361  0.846730
10           (64, 64)  0.0100               0.010        50  0.713778  0.693746     0.762643     0.646076  0.598539  0.797328
11           (64, 64)  0.0100               0.010       100  0.713778  0.693746     0.762643     0.646076  0.598539  0.797328
12         (128, 128)  0.0001               0.001        50  0.798859  0.801677     0.827366     0.776702  0.782705  0.822317
13         (128, 128)  0.0001               0.001       100  0.798859  0.801677     0.827366     0.776702  0.782705  0.822317
14         (128, 128)  0.0001               0.010        50  0.818536  0.815133     0.865292     0.772374  0.764183  0.870567
15         (128, 128)  0.0001               0.010       100  0.814635  0.814747     0.850144     0.782134  0.782388  0.849954
16         (128, 128)  0.0010               0.001        50  0.816688  0.820924     0.840602     0.801085  0.809907  0.832911
17         (128, 128)  0.0010               0.001       100  0.816688  0.820924     0.840602     0.801085  0.809907  0.832911
18         (128, 128)  0.0010               0.010        50  0.761398  0.760148     0.796833     0.727130  0.724386  0.799056
19         (128, 128)  0.0010               0.010       100  0.761398  0.760148     0.796833     0.727130  0.724386  0.799056
20         (128, 128)  0.0100               0.001        50  0.833210  0.838462     0.852188     0.824152  0.834780  0.842469
21         (128, 128)  0.0100               0.001       100  0.850263  0.852195     0.879457     0.825809  0.830123  0.876209
22         (128, 128)  0.0100               0.010        50  0.716747  0.688065     0.776869     0.634172  0.563082  0.824044
23         (128, 128)  0.0100               0.010       100  0.716747  0.688065     0.776869     0.634172  0.563082  0.824044
24         (256, 256)  0.0001               0.001        50  0.806271  0.813258     0.824588     0.801228  0.814987  0.811377
25         (256, 256)  0.0001               0.001       100  0.806271  0.813258     0.824588     0.801228  0.814987  0.811377
26         (256, 256)  0.0001               0.010        50  0.820860  0.814913     0.876887     0.765075  0.750106  0.885421
27         (256, 256)  0.0001               0.010       100  0.820860  0.814913     0.876887     0.765075  0.750106  0.885421
28         (256, 256)  0.0010               0.001        50  0.818539  0.824454     0.837736     0.810545  0.822396  0.826693
29         (256, 256)  0.0010               0.001       100  0.818539  0.824454     0.837736     0.810545  0.822396  0.826693
30         (256, 256)  0.0010               0.010        50  0.757720  0.764615     0.781522     0.747201  0.761008  0.768540
31         (256, 256)  0.0010               0.010       100  0.757720  0.764615     0.781522     0.747201  0.761008  0.768540
32         (256, 256)  0.0100               0.001        50  0.841012  0.844694     0.864236     0.824989  0.832769  0.857669
33         (256, 256)  0.0100               0.001       100  0.841576  0.847011     0.858586     0.834806  0.845682  0.848457
34         (256, 256)  0.0100               0.010        50  0.700096  0.689279     0.737484     0.650960  0.626799  0.757255
35         (256, 256)  0.0100               0.010       100  0.700096  0.689279     0.737484     0.650960  0.626799  0.757255

Best Parameters for MLPClassifier:
{'hidden_layer_sizes': (128, 128), 'alpha': 0.01, 'learning_rate_init': 0.001, 'max_iter': 100, 'f1_score': 0.850262599173092, 'accuracy': 0.8521950143392897, 'precision_0': 0.8794572774164611, 'precision_1': 0.8258085522031691, 'recall_0': 0.8301227773073666, 'recall_1': 0.8762091202210963}

Classification Report for Best MLPClassifier:
              precision    recall  f1-score   support

           0       0.88      0.83      0.85      9448
           1       0.83      0.88      0.85      8684

    accuracy                           0.85     18132
   macro avg       0.85      0.85      0.85     18132
weighted avg       0.85      0.85      0.85     18132



##############################
XGBClassifier Full Results
##############################
    n_estimators  max_depth  learning_rate  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            100          5           0.01  0.635589  0.636554     0.663651     0.611383  0.613357  0.661792
1            100          5           0.10  0.778765  0.778899     0.812766     0.747695  0.747989  0.812529
2            100          5           0.20  0.826160  0.828921     0.853656     0.804694  0.810648  0.848802
3            100         10           0.01  0.722623  0.721873     0.755067     0.691692  0.690093  0.756449
4            100         10           0.10  0.857867  0.860909     0.881724     0.840066  0.846634  0.876439
5            100         10           0.20  0.876329  0.879440     0.897265     0.861321  0.868014  0.891870
6            100         15           0.01  0.744997  0.743492     0.779643     0.711041  0.707769  0.782358
7            100         15           0.10  0.833794  0.837580     0.857426     0.817598  0.825572  0.850645
8            100         15           0.20  0.841848  0.846514     0.861482     0.831033  0.840601  0.852948
9            200          5           0.01  0.659773  0.649790     0.690015     0.616934  0.595364  0.709005
10           200          5           0.10  0.840872  0.843426     0.868189     0.819155  0.824725  0.863772
11           200          5           0.20  0.879008  0.881591     0.902436     0.860722  0.866427  0.898088
12           200         10           0.01  0.745914  0.738529     0.788525     0.697644  0.680779  0.801359
13           200         10           0.10  0.884186  0.886830     0.906463     0.867058  0.872883  0.902004
14           200         10           0.20  0.886209  0.889146     0.906359     0.871604  0.877964  0.901313
15           200         15           0.01  0.769947  0.768476     0.806373     0.734525  0.731266  0.808959
16           200         15           0.10  0.852856  0.856662     0.874140     0.838846  0.846846  0.867342
17           200         15           0.20  0.862785  0.866645     0.882315     0.850526  0.858594  0.875403
18           300          5           0.01  0.682004  0.669921     0.716573     0.633126  0.606372  0.739060
19           300          5           0.10  0.870234  0.872546     0.896192     0.849205  0.854361  0.892331
20           300          5           0.20  0.900023  0.902272     0.922129     0.882301  0.887384  0.918471
21           300         10           0.01  0.772769  0.768972     0.813745     0.730489  0.721846  0.820244
22           300         10           0.10  0.890546  0.893117     0.912184     0.873864  0.879551  0.907877
23           300         10           0.20  0.890810  0.893448     0.911971     0.874695  0.880504  0.907531
24           300         15           0.01  0.788391  0.788882     0.822026     0.758133  0.759208  0.821165
25           300         15           0.10  0.864538  0.867803     0.886526     0.848851  0.855843  0.880815
26           300         15           0.20  0.873688  0.877178     0.893087     0.860847  0.868226  0.886918

Best Parameters for XGBClassifier:
{'n_estimators': 300.0, 'max_depth': 5.0, 'learning_rate': 0.2, 'f1_score': 0.9000225682690137, 'accuracy': 0.9022722258989632, 'precision_0': 0.9221293444786626, 'precision_1': 0.8823008849557522, 'recall_0': 0.8873835732430144, 'recall_1': 0.9184707508060801}

Classification Report for Best XGBClassifier:
              precision    recall  f1-score   support

           0       0.92      0.89      0.90      9448
           1       0.88      0.92      0.90      8684

    accuracy                           0.90     18132
   macro avg       0.90      0.90      0.90     18132
weighted avg       0.90      0.90      0.90     18132



##############################
LGBMClassifier Full Results
##############################
    n_estimators  max_depth  learning_rate  is_unbalance  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            100          5           0.01          True  0.666215  0.647143     0.699399     0.609023  0.566152  0.735260
1            100          5           0.01         False  0.645098  0.642455     0.673412     0.614839  0.609335  0.678489
2            100          5           0.10          True  0.776622  0.766656     0.831238     0.717071  0.692845  0.846960
3            100          5           0.10         False  0.772071  0.772612     0.805088     0.742477  0.743649  0.804123
4            100          5           0.20          True  0.829437  0.828590     0.868862     0.792304  0.790326  0.870221
5            100          5           0.20         False  0.825827  0.828315     0.854188     0.803134  0.808531  0.849839
6            100         10           0.01          True  0.675353  0.656464     0.710971     0.616871  0.574090  0.746085
7            100         10           0.01         False  0.663311  0.656960     0.693479     0.625843  0.612299  0.705550
8            100         10           0.10          True  0.811385  0.808129     0.856528     0.766622  0.758891  0.861700
9            100         10           0.10         False  0.819889  0.822799     0.847276     0.798798  0.805038  0.842123
10           100         10           0.20          True  0.854900  0.856276     0.886292     0.827620  0.830758  0.884040
11           100         10           0.20         False  0.853373  0.857214     0.874440     0.839630  0.847693  0.867573
12           100         15           0.01          True  0.675910  0.657070     0.711664     0.617406  0.574725  0.746661
13           100         15           0.01         False  0.661999  0.656519     0.691895     0.626052  0.614416  0.702326
14           100         15           0.10          True  0.817113  0.813755     0.863489     0.771291  0.763230  0.868724
15           100         15           0.10         False  0.820585  0.824068     0.846282     0.802001  0.809378  0.840051
16           100         15           0.20          True  0.854618  0.855725     0.887214     0.825886  0.828429  0.885421
17           100         15           0.20         False  0.854424  0.858262     0.875355     0.840803  0.848857  0.868494
18           200          5           0.01          True  0.686422  0.653927     0.733137     0.606339  0.528048  0.790880
19           200          5           0.01         False  0.660382  0.650287     0.690731     0.617302  0.595470  0.709926
20           200          5           0.10          True  0.836901  0.835705     0.878260     0.797725  0.794877  0.880124
21           200          5           0.10         False  0.832602  0.835981     0.857680     0.814358  0.821550  0.851681
22           200          5           0.20          True  0.879951  0.881315     0.910349     0.853387  0.856583  0.908222
23           200          5           0.20         False  0.881566  0.884238     0.904102     0.864255  0.870131  0.899585
24           200         10           0.01          True  0.703142  0.678028     0.752448     0.629576  0.569433  0.796177
25           200         10           0.01         False  0.690880  0.680344     0.726382     0.643453  0.620131  0.745854
26           200         10           0.10          True  0.863846  0.864935     0.896364     0.835107  0.837638  0.894634
27           200         10           0.10         False  0.865580  0.868961     0.886871     0.850756  0.857959  0.880930
28           200         10           0.20          True  0.893989  0.895544     0.922011     0.869745  0.873412  0.919622
29           200         10           0.20         False  0.890499  0.893283     0.910771     0.875487  0.881562  0.906034
30           200         15           0.01          True  0.703281  0.677311     0.753417     0.628364  0.565940  0.798480
31           200         15           0.01         False  0.692419  0.682495     0.727868     0.645869  0.623942  0.746200
32           200         15           0.10          True  0.864895  0.865431     0.900183     0.832978  0.834251  0.899355
33           200         15           0.10         False  0.866927  0.870340     0.887857     0.852499  0.859759  0.881852
34           200         15           0.20          True  0.886393  0.888429     0.912088     0.865066  0.869708  0.908798
35           200         15           0.20         False  0.894570  0.897198     0.915061     0.879057  0.884843  0.910640
36           300          5           0.01          True  0.702115  0.669093     0.758316     0.617123  0.535669  0.814256
37           300          5           0.01         False  0.682293  0.671079     0.716559     0.634814  0.610076  0.737448
38           300          5           0.10          True  0.869719  0.870229     0.905197     0.837581  0.838802  0.904422
39           300          5           0.10         False  0.869952  0.872656     0.893941     0.851395  0.857324  0.889337
40           300          5           0.20          True  0.902724  0.904092     0.931200     0.877733  0.881033  0.929180
41           300          5           0.20         False  0.903128  0.905030     0.927323     0.882864  0.887278  0.924344
42           300         10           0.01          True  0.723610  0.701081     0.779489     0.649368  0.594517  0.817020
43           300         10           0.01         False  0.714492  0.705659     0.753047     0.667200  0.647439  0.769000
44           300         10           0.10          True  0.882921  0.884348     0.912746     0.856942  0.860288  0.910525
45           300         10           0.10         False  0.882616  0.885561     0.903381     0.867452  0.873836  0.898319
46           300         10           0.20          True  0.904797  0.906409     0.931042     0.882179  0.886008  0.928604
47           300         10           0.20         False  0.906190  0.908229     0.928729     0.887674  0.892358  0.925495
48           300         15           0.01          True  0.724346  0.699702     0.783378     0.646310  0.585627  0.823814
49           300         15           0.01         False  0.716402  0.709629     0.753484     0.673009  0.658023  0.765776
50           300         15           0.10          True  0.883942  0.885286     0.914212     0.857437  0.860605  0.912137
51           300         15           0.10         False  0.886573  0.889532     0.906520     0.872201  0.878599  0.901428
52           300         15           0.20          True  0.903800  0.905747     0.927522     0.884044  0.888548  0.924459
53           300         15           0.20         False  0.905360  0.907677     0.925833     0.889271  0.894475  0.922041

Best Parameters for LGBMClassifier:
{'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.2, 'is_unbalance': False, 'f1_score': 0.9061901003495321, 'accuracy': 0.9082285462166336, 'precision_0': 0.928728794888742, 'precision_1': 0.8876739562624254, 'recall_0': 0.8923581710414903, 'recall_1': 0.9254951635191156}

Classification Report for Best LGBMClassifier:
              precision    recall  f1-score   support

           0       0.93      0.89      0.91      9448
           1       0.89      0.93      0.91      8684

    accuracy                           0.91     18132
   macro avg       0.91      0.91      0.91     18132
weighted avg       0.91      0.91      0.91     18132



##############################
Ensemble Model Performance
##############################
Ensemble Training Time: 341.85 seconds
Ensemble Classification Report:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92      9448
           1       0.89      0.94      0.91      8684

    accuracy                           0.91     18132
   macro avg       0.91      0.92      0.91     18132
weighted avg       0.92      0.91      0.91     18132

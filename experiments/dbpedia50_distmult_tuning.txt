##############################
MLPClassifier Full Results
##############################
   hidden_layer_sizes   alpha  learning_rate_init  max_iter  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            (64, 64)  0.0001               0.001        25  0.610574  0.715413     0.688420     0.783179  0.888528  0.500311
1            (64, 64)  0.0001               0.001        50  0.610574  0.715413     0.688420     0.783179  0.888528  0.500311
2            (64, 64)  0.0001               0.001       100  0.610574  0.715413     0.688420     0.783179  0.888528  0.500311
3            (64, 64)  0.0001               0.010        25  0.556863  0.702673     0.665654     0.830154  0.931017  0.418944
4            (64, 64)  0.0001               0.010        50  0.563802  0.701288     0.667758     0.808116  0.917271  0.432919
5            (64, 64)  0.0001               0.010       100  0.563802  0.701288     0.667758     0.808116  0.917271  0.432919
6            (64, 64)  0.0001               0.100        25  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
7            (64, 64)  0.0001               0.100        50  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
8            (64, 64)  0.0001               0.100       100  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
9            (64, 64)  0.0010               0.001        25  0.615444  0.722753     0.690926     0.806647  0.904024  0.497516
10           (64, 64)  0.0010               0.001        50  0.615444  0.722753     0.690926     0.806647  0.904024  0.497516
11           (64, 64)  0.0010               0.001       100  0.615444  0.722753     0.690926     0.806647  0.904024  0.497516
12           (64, 64)  0.0010               0.010        25  0.145839  0.574990     0.568049     0.702413  0.972257  0.081366
13           (64, 64)  0.0010               0.010        50  0.145839  0.574990     0.568049     0.702413  0.972257  0.081366
14           (64, 64)  0.0010               0.010       100  0.145839  0.574990     0.568049     0.702413  0.972257  0.081366
15           (64, 64)  0.0010               0.100        25  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
16           (64, 64)  0.0010               0.100        50  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
17           (64, 64)  0.0010               0.100       100  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
18           (64, 64)  0.0100               0.001        25  0.601208  0.716521     0.684062     0.806587  0.907523  0.479193
19           (64, 64)  0.0100               0.001        50  0.601208  0.716521     0.684062     0.806587  0.907523  0.479193
20           (64, 64)  0.0100               0.001       100  0.601208  0.716521     0.684062     0.806587  0.907523  0.479193
21           (64, 64)  0.0100               0.010        25  0.247477  0.566265     0.569199     0.546709  0.893277  0.159938
22           (64, 64)  0.0100               0.010        50  0.247477  0.566265     0.569199     0.546709  0.893277  0.159938
23           (64, 64)  0.0100               0.010       100  0.247477  0.566265     0.569199     0.546709  0.893277  0.159938
24           (64, 64)  0.0100               0.100        25  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
25           (64, 64)  0.0100               0.100        50  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
26           (64, 64)  0.0100               0.100       100  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
27         (128, 128)  0.0001               0.001        25  0.603502  0.717768     0.685154     0.807813  0.907773  0.481677
28         (128, 128)  0.0001               0.001        50  0.603502  0.717768     0.685154     0.807813  0.907773  0.481677
29         (128, 128)  0.0001               0.001       100  0.603502  0.717768     0.685154     0.807813  0.907773  0.481677
30         (128, 128)  0.0001               0.010        25  0.244251  0.604071     0.585761     0.820604  0.974756  0.143478
31         (128, 128)  0.0001               0.010        50  0.244251  0.604071     0.585761     0.820604  0.974756  0.143478
32         (128, 128)  0.0001               0.010       100  0.244251  0.604071     0.585761     0.820604  0.974756  0.143478
33         (128, 128)  0.0001               0.100        25  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
34         (128, 128)  0.0001               0.100        50  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
35         (128, 128)  0.0001               0.100       100  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
36         (128, 128)  0.0010               0.001        25  0.615895  0.727600     0.691165     0.829563  0.919020  0.489752
37         (128, 128)  0.0010               0.001        50  0.615895  0.727600     0.691165     0.829563  0.919020  0.489752
38         (128, 128)  0.0010               0.001       100  0.615895  0.727600     0.691165     0.829563  0.919020  0.489752
39         (128, 128)  0.0010               0.010        25  0.115950  0.575544     0.567106     0.813765  0.988503  0.062422
40         (128, 128)  0.0010               0.010        50  0.115950  0.575544     0.567106     0.813765  0.988503  0.062422
41         (128, 128)  0.0010               0.010       100  0.115950  0.575544     0.567106     0.813765  0.988503  0.062422
42         (128, 128)  0.0010               0.100        25  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
43         (128, 128)  0.0010               0.100        50  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
44         (128, 128)  0.0010               0.100       100  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
45         (128, 128)  0.0100               0.001        25  0.593589  0.715552     0.680713     0.817884  0.916521  0.465839
46         (128, 128)  0.0100               0.001        50  0.601478  0.708766     0.683772     0.771512  0.882529  0.492857
47         (128, 128)  0.0100               0.001       100  0.601478  0.708766     0.683772     0.771512  0.882529  0.492857
48         (128, 128)  0.0100               0.010        25  0.176625  0.564880     0.564830     0.565436  0.935266  0.104658
49         (128, 128)  0.0100               0.010        50  0.176625  0.564880     0.564830     0.565436  0.935266  0.104658
50         (128, 128)  0.0100               0.010       100  0.176625  0.564880     0.564830     0.565436  0.935266  0.104658
51         (128, 128)  0.0100               0.100        25  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
52         (128, 128)  0.0100               0.100        50  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
53         (128, 128)  0.0100               0.100       100  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
54         (256, 256)  0.0001               0.001        25  0.632088  0.725938     0.699723     0.787402  0.885279  0.527950
55         (256, 256)  0.0001               0.001        50  0.632088  0.725938     0.699723     0.787402  0.885279  0.527950
56         (256, 256)  0.0001               0.001       100  0.632088  0.725938     0.699723     0.787402  0.885279  0.527950
57         (256, 256)  0.0001               0.010        25  0.132420  0.579006     0.569266     0.816901  0.987003  0.072050
58         (256, 256)  0.0001               0.010        50  0.132420  0.579006     0.569266     0.816901  0.987003  0.072050
59         (256, 256)  0.0001               0.010       100  0.132420  0.579006     0.569266     0.816901  0.987003  0.072050
60         (256, 256)  0.0001               0.100        25  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
61         (256, 256)  0.0001               0.100        50  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
62         (256, 256)  0.0001               0.100       100  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
63         (256, 256)  0.0010               0.001        25  0.624431  0.725800     0.695492     0.802144  0.898525  0.511180
64         (256, 256)  0.0010               0.001        50  0.632786  0.733832     0.699635     0.822244  0.910522  0.514286
65         (256, 256)  0.0010               0.001       100  0.632786  0.733832     0.699635     0.822244  0.910522  0.514286
66         (256, 256)  0.0010               0.010        25  0.118829  0.574851     0.566911     0.784091  0.985754  0.064286
67         (256, 256)  0.0010               0.010        50  0.118829  0.574851     0.566911     0.784091  0.985754  0.064286
68         (256, 256)  0.0010               0.010       100  0.118829  0.574851     0.566911     0.784091  0.985754  0.064286
69         (256, 256)  0.0010               0.100        25  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
70         (256, 256)  0.0010               0.100        50  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
71         (256, 256)  0.0010               0.100       100  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
72         (256, 256)  0.0100               0.001        25  0.606375  0.716106     0.686413     0.794266  0.897776  0.490373
73         (256, 256)  0.0100               0.001        50  0.606375  0.716106     0.686413     0.794266  0.897776  0.490373
74         (256, 256)  0.0100               0.001       100  0.606375  0.716106     0.686413     0.794266  0.897776  0.490373
75         (256, 256)  0.0100               0.010        25  0.157967  0.573328     0.567827     0.658314  0.962509  0.089752
76         (256, 256)  0.0100               0.010        50  0.157967  0.573328     0.567827     0.658314  0.962509  0.089752
77         (256, 256)  0.0100               0.010       100  0.157967  0.573328     0.567827     0.658314  0.962509  0.089752
78         (256, 256)  0.0100               0.100        25  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
79         (256, 256)  0.0100               0.100        50  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000
80         (256, 256)  0.0100               0.100       100  0.000000  0.554078     0.554078     0.000000  1.000000  0.000000

Best Parameters for MLPClassifier:
{'hidden_layer_sizes': (256, 256), 'alpha': 0.001, 'learning_rate_init': 0.001, 'max_iter': 100, 'f1_score': 0.6327856324035155, 'accuracy': 0.7338318792411024, 'precision_0': 0.6996351065872863, 'precision_1': 0.8222442899702085, 'recall_0': 0.9105223694076481, 'recall_1': 0.5142857142857142}

Classification Report for Best MLPClassifier:
              precision    recall  f1-score   support

           0       0.70      0.91      0.79      4001
           1       0.82      0.51      0.63      3220

    accuracy                           0.73      7221
   macro avg       0.76      0.71      0.71      7221
weighted avg       0.75      0.73      0.72      7221



##############################
XGBClassifier Full Results
##############################
    n_estimators  max_depth  learning_rate  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            100          5           0.01  0.502517  0.685224     0.647139     0.851001  0.949763  0.356522
1            100          5           0.10  0.736498  0.787841     0.766804     0.825366  0.886778  0.664907
2            100          5           0.20  0.835614  0.855283     0.861900     0.846669  0.879780  0.824845
3            100         10           0.01  0.624438  0.733970     0.695268     0.842744  0.925519  0.495963
4            100         10           0.10  0.821009  0.846143     0.841285     0.853030  0.890277  0.791304
5            100         10           0.20  0.840630  0.861377     0.860577     0.862463  0.894776  0.819876
6            100         15           0.01  0.679252  0.750589     0.727932     0.796242  0.878030  0.592236
7            100         15           0.10  0.788097  0.820523     0.812717     0.832182  0.878530  0.748447
8            100         15           0.20  0.806134  0.833680     0.829722     0.839328  0.880530  0.775466
9            200          5           0.01  0.563043  0.707243     0.668392     0.841780  0.936016  0.422981
10           200          5           0.10  0.839404  0.859576     0.861885     0.856496  0.889028  0.822981
11           200          5           0.20  0.874420  0.887550     0.901132     0.870918  0.895276  0.877950
12           200         10           0.01  0.669525  0.754466     0.719464     0.837296  0.912772  0.557764
13           200         10           0.10  0.850421  0.869686     0.868675     0.871052  0.901025  0.830745
14           200         10           0.20  0.860892  0.878133     0.879222     0.876690  0.904274  0.845652
15           200         15           0.01  0.717521  0.770253     0.756349     0.794195  0.863534  0.654348
16           200         15           0.10  0.813642  0.840327     0.834744     0.848332  0.887528  0.781677
17           200         15           0.20  0.824716  0.848359     0.846447     0.851008  0.887278  0.800000
18           300          5           0.01  0.596435  0.720953     0.682269     0.839820  0.929018  0.462422
19           300          5           0.10  0.867122  0.881457     0.893223     0.866853  0.892777  0.867391
20           300          5           0.20  0.888614  0.900291     0.912497     0.885327  0.907023  0.891925
21           300         10           0.01  0.719731  0.781055     0.752083     0.838496  0.902274  0.630435
22           300         10           0.10  0.859760  0.877025     0.878803     0.874679  0.902524  0.845342
23           300         10           0.20  0.865324  0.881457     0.884945     0.876913  0.903524  0.854037
24           300         15           0.01  0.738999  0.784794     0.772677     0.804682  0.866533  0.683230
25           300         15           0.10  0.824891  0.849190     0.844860     0.855285  0.891527  0.796584
26           300         15           0.20  0.839534  0.860823     0.858545     0.863950  0.896526  0.816460

Best Parameters for XGBClassifier:
{'n_estimators': 300.0, 'max_depth': 5.0, 'learning_rate': 0.2, 'f1_score': 0.8886138613861386, 'accuracy': 0.9002908184461986, 'precision_0': 0.9124968569273322, 'precision_1': 0.8853267570900123, 'recall_0': 0.9070232441889527, 'recall_1': 0.8919254658385093}

Classification Report for Best XGBClassifier:
              precision    recall  f1-score   support

           0       0.91      0.91      0.91      4001
           1       0.89      0.89      0.89      3220

    accuracy                           0.90      7221
   macro avg       0.90      0.90      0.90      7221
weighted avg       0.90      0.90      0.90      7221



##############################
LGBMClassifier Full Results
##############################
    n_estimators  max_depth  learning_rate  is_unbalance  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            100          5           0.01          True  0.558519  0.692840     0.664574     0.777716  0.899775  0.435714
1            100          5           0.01         False  0.504477  0.685778     0.647731     0.849890  0.949013  0.358696
2            100          5           0.10          True  0.760044  0.789087     0.802638     0.771346  0.821295  0.749068
3            100          5           0.10         False  0.724497  0.780086     0.757965     0.820755  0.886028  0.648447
4            100          5           0.20          True  0.838009  0.851267     0.884020     0.814663  0.842039  0.862733
5            100          5           0.20         False  0.834807  0.855145     0.859577     0.849293  0.882779  0.820807
6            100         10           0.01          True  0.594511  0.709459     0.680653     0.787103  0.896026  0.477640
7            100         10           0.01         False  0.527312  0.695610     0.655673     0.857343  0.949013  0.380745
8            100         10           0.10          True  0.826788  0.842404     0.869801     0.810746  0.841540  0.843478
9            100         10           0.10         False  0.788570  0.822739     0.810121     0.842272  0.888278  0.741304
10           100         10           0.20          True  0.860919  0.871763     0.906423     0.833624  0.857036  0.890062
11           100         10           0.20         False  0.844423  0.863038     0.868756     0.855595  0.886778  0.833540
12           100         15           0.01          True  0.591647  0.707520     0.679256     0.783811  0.894526  0.475155
13           100         15           0.01         False  0.528805  0.696441     0.656245     0.858939  0.949513  0.381988
14           100         15           0.10          True  0.824692  0.840327     0.868530     0.807864  0.838790  0.842236
15           100         15           0.10         False  0.793030  0.825647     0.814594     0.842473  0.887278  0.749068
16           100         15           0.20          True  0.858042  0.868855     0.905040     0.829325  0.852787  0.888820
17           100         15           0.20         False  0.859561  0.875917     0.882295     0.867722  0.895526  0.851553
18           200          5           0.01          True  0.622700  0.713198     0.694871     0.753195  0.860035  0.530745
19           200          5           0.01         False  0.553891  0.703088     0.664774     0.839218  0.936266  0.413354
20           200          5           0.10          True  0.843406  0.856114     0.889122     0.819327  0.845789  0.868944
21           200          5           0.10         False  0.824690  0.847390     0.848857     0.845401  0.881530  0.804969
22           200          5           0.20          True  0.880456  0.889627     0.924483     0.851465  0.872032  0.911491
23           200          5           0.20         False  0.878554  0.891151     0.905014     0.874231  0.897776  0.882919
24           200         10           0.01          True  0.648504  0.726631     0.710052     0.760017  0.856286  0.565528
25           200         10           0.01         False  0.591329  0.718045     0.679978     0.835982  0.927768  0.457453
26           200         10           0.10          True  0.867557  0.877718     0.913090     0.838990  0.861285  0.898137
27           200         10           0.10         False  0.856652  0.872871     0.881843     0.861495  0.889778  0.851863
28           200         10           0.20          True  0.878994  0.889351     0.917144     0.857819  0.879780  0.901242
29           200         10           0.20         False  0.873161  0.886581     0.899347     0.870868  0.895526  0.875466
30           200         15           0.01          True  0.649341  0.727185     0.710554     0.760634  0.856536  0.566460
31           200         15           0.01         False  0.592206  0.718876     0.680395     0.838453  0.929018  0.457764
32           200         15           0.10          True  0.867607  0.877856     0.912675     0.839628  0.862034  0.897516
33           200         15           0.10         False  0.854574  0.872317     0.875396     0.868269  0.897276  0.841304
34           200         15           0.20          True  0.883215  0.892951     0.922292     0.859959  0.881030  0.907764
35           200         15           0.20         False  0.878961  0.891566     0.905086     0.875038  0.898525  0.882919
36           300          5           0.01          True  0.648997  0.721230     0.711220     0.739960  0.836541  0.577950
37           300          5           0.01         False  0.592533  0.718876     0.680528     0.837684  0.928518  0.458385
38           300          5           0.10          True  0.867771  0.878410     0.910813     0.842398  0.865284  0.894720
39           300          5           0.10         False  0.858785  0.874117     0.886114     0.859186  0.886778  0.858385
40           300          5           0.20          True  0.890665  0.899598     0.929921     0.865729  0.885529  0.917081
41           300          5           0.20         False  0.890529  0.901399     0.917704     0.881851  0.903024  0.899379
42           300         10           0.01          True  0.690343  0.746434     0.739620     0.757891  0.837041  0.633851
43           300         10           0.01         False  0.625895  0.732309     0.696056     0.830508  0.917521  0.502174
44           300         10           0.10          True  0.887123  0.895721     0.930769     0.857433  0.877031  0.918944
45           300         10           0.10         False  0.876623  0.889489     0.903096     0.872845  0.896776  0.880435
46           300         10           0.20          True  0.887772  0.897937     0.921270     0.870929  0.892027  0.905280
47           300         10           0.20         False  0.880816  0.893228     0.906596     0.876885  0.900025  0.884783
48           300         15           0.01          True  0.690352  0.745326     0.740116     0.753954  0.832792  0.636646
49           300         15           0.01         False  0.624927  0.732032     0.695570     0.831356  0.918270  0.500621
50           300         15           0.10          True  0.881850  0.891012     0.925132     0.853531  0.874031  0.912112
51           300         15           0.10         False  0.873913  0.887550     0.898525     0.873913  0.898525  0.873913
52           300         15           0.20          True  0.887038  0.896690     0.924159     0.865544  0.886278  0.909627
53           300         15           0.20         False  0.888337  0.900291     0.910844     0.887237  0.909023  0.889441

Best Parameters for LGBMClassifier:
{'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2, 'is_unbalance': True, 'f1_score': 0.8906650580606243, 'accuracy': 0.8995983935742972, 'precision_0': 0.9299212598425197, 'precision_1': 0.8657285253591323, 'recall_0': 0.8855286178455386, 'recall_1': 0.9170807453416149}

Classification Report for Best LGBMClassifier:
              precision    recall  f1-score   support

           0       0.93      0.89      0.91      4001
           1       0.87      0.92      0.89      3220

    accuracy                           0.90      7221
   macro avg       0.90      0.90      0.90      7221
weighted avg       0.90      0.90      0.90      7221



##############################
Ensemble Model Performance
##############################
Ensemble Accuracy: 0.8977
Ensemble Classification Report:
              precision    recall  f1-score   support

           0       0.91      0.90      0.91      4001
           1       0.88      0.89      0.89      3220

    accuracy                           0.90      7221
   macro avg       0.90      0.90      0.90      7221
weighted avg       0.90      0.90      0.90      7221

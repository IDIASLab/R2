##############################
MLPClassifier Full Results
##############################
   hidden_layer_sizes   alpha  learning_rate_init  max_iter  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            (64, 64)  0.0001               0.001        50  0.681242  0.689874     0.712465     0.666998  0.684211  0.696108
1            (64, 64)  0.0001               0.001       100  0.681242  0.689874     0.712465     0.666998  0.684211  0.696108
2            (64, 64)  0.0001               0.010        50  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
3            (64, 64)  0.0001               0.010       100  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
4            (64, 64)  0.0010               0.001        50  0.670031  0.677430     0.701969     0.653039  0.667887  0.687932
5            (64, 64)  0.0010               0.001       100  0.670031  0.677430     0.701969     0.653039  0.667887  0.687932
6            (64, 64)  0.0010               0.010        50  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
7            (64, 64)  0.0010               0.010       100  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
8            (64, 64)  0.0100               0.001        50  0.610743  0.626556     0.645623     0.606171  0.636706  0.615385
9            (64, 64)  0.0100               0.001       100  0.610743  0.626556     0.645623     0.606171  0.636706  0.615385
10           (64, 64)  0.0100               0.010        50  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
11           (64, 64)  0.0100               0.010       100  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
12         (128, 128)  0.0001               0.001        50  0.686610  0.686421     0.721204     0.654891  0.654494  0.721557
13         (128, 128)  0.0001               0.001       100  0.702265  0.701661     0.737944     0.668960  0.667678  0.739060
14         (128, 128)  0.0001               0.010        50  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
15         (128, 128)  0.0001               0.010       100  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
16         (128, 128)  0.0010               0.001        50  0.662248  0.661970     0.695582     0.631529  0.630951  0.696108
17         (128, 128)  0.0010               0.001       100  0.662248  0.661970     0.695582     0.631529  0.630951  0.696108
18         (128, 128)  0.0010               0.010        50  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
19         (128, 128)  0.0010               0.010       100  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
20         (128, 128)  0.0100               0.001        50  0.634341  0.613453     0.663962     0.577036  0.530920  0.704284
21         (128, 128)  0.0100               0.001       100  0.634341  0.613453     0.663962     0.577036  0.530920  0.704284
22         (128, 128)  0.0100               0.010        50  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
23         (128, 128)  0.0100               0.010       100  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
24         (256, 256)  0.0001               0.001        50  0.646770  0.652870     0.679187     0.627245  0.639531  0.667550
25         (256, 256)  0.0001               0.001       100  0.646770  0.652870     0.679187     0.627245  0.639531  0.667550
26         (256, 256)  0.0001               0.010        50  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
27         (256, 256)  0.0001               0.010       100  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
28         (256, 256)  0.0010               0.001        50  0.651678  0.636478     0.685472     0.599150  0.565763  0.714302
29         (256, 256)  0.0010               0.001       100  0.651678  0.636478     0.685472     0.599150  0.565763  0.714302
30         (256, 256)  0.0010               0.010        50  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
31         (256, 256)  0.0010               0.010       100  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
32         (256, 256)  0.0100               0.001        50  0.632416  0.617181     0.662336     0.582469  0.549440  0.691732
33         (256, 256)  0.0100               0.001       100  0.632416  0.617181     0.662336     0.582469  0.549440  0.691732
34         (256, 256)  0.0100               0.010        50  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000
35         (256, 256)  0.0100               0.010       100  0.000000  0.523930     0.523930     0.000000  1.000000  0.000000

Best Parameters for MLPClassifier:
{'hidden_layer_sizes': (128, 128), 'alpha': 0.0001, 'learning_rate_init': 0.001, 'max_iter': 100, 'f1_score': 0.7022650180544917, 'accuracy': 0.7016610931418233, 'precision_0': 0.7379437955360241, 'precision_1': 0.6689597665207422, 'recall_0': 0.6676781416762583, 'recall_1': 0.739060340856748}

Classification Report for Best MLPClassifier:
              precision    recall  f1-score   support

           0       0.74      0.67      0.70      9557
           1       0.67      0.74      0.70      8684

    accuracy                           0.70     18241
   macro avg       0.70      0.70      0.70     18241
weighted avg       0.71      0.70      0.70     18241



##############################
XGBClassifier Full Results
##############################
    n_estimators  max_depth  learning_rate  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            100          5           0.01  0.480660  0.627542     0.599798     0.714870  0.868787  0.362045
1            100          5           0.10  0.772881  0.790636     0.783778     0.799164  0.829130  0.748273
2            100          5           0.20  0.824374  0.833781     0.837698     0.829371  0.846814  0.819438
3            100         10           0.01  0.642662  0.698591     0.675882     0.737690  0.816051  0.569323
4            100         10           0.10  0.832079  0.841017     0.844744     0.836827  0.853406  0.827384
5            100         10           0.20  0.847767  0.855545     0.859921     0.850667  0.865230  0.844887
6            100         15           0.01  0.703636  0.734554     0.722679     0.750980  0.800565  0.661907
7            100         15           0.10  0.798054  0.811140     0.809751     0.812776  0.835932  0.783855
8            100         15           0.20  0.809846  0.821282     0.821900     0.820567  0.841164  0.799401
9            200          5           0.01  0.594786  0.674470     0.647461     0.729983  0.831328  0.501842
10           200          5           0.10  0.832328  0.841566     0.844050     0.838751  0.855708  0.826002
11           200          5           0.20  0.867754  0.872924     0.885176     0.859905  0.870357  0.875749
12           200         10           0.01  0.708203  0.739652     0.725728     0.759189  0.808727  0.663634
13           200         10           0.10  0.859321  0.866455     0.870538     0.861909  0.875275  0.856748
14           200         10           0.20  0.862399  0.869196     0.874074     0.863794  0.876635  0.861009
15           200         15           0.01  0.739058  0.760101     0.755096     0.766386  0.802344  0.713611
16           200         15           0.10  0.818644  0.829998     0.828516     0.831729  0.851836  0.805965
17           200         15           0.20  0.834001  0.843156     0.845511     0.840486  0.857277  0.827614
18           300          5           0.01  0.647554  0.701442     0.679160     0.739214  0.815319  0.576117
19           300          5           0.10  0.856535  0.863275     0.870139     0.855747  0.868683  0.857324
20           300          5           0.20  0.881105  0.885368     0.899764     0.870268  0.879146  0.892216
21           300         10           0.01  0.747750  0.769530     0.760893     0.780631  0.816784  0.717526
22           300         10           0.10  0.867205  0.873691     0.878747     0.868105  0.880402  0.866306
23           300         10           0.20  0.868423  0.874842     0.879883     0.869274  0.881448  0.867573
24           300         15           0.01  0.758045  0.775780     0.772777     0.779440  0.810296  0.737794
25           300         15           0.10  0.831466  0.841675     0.840637     0.842877  0.861044  0.820359
26           300         15           0.20  0.845864  0.854449     0.855773     0.852945  0.868578  0.838899

Best Parameters for XGBClassifier:
{'n_estimators': 300.0, 'max_depth': 5.0, 'learning_rate': 0.2, 'f1_score': 0.8811053619150508, 'accuracy': 0.8853681267474371, 'precision_0': 0.8997644035125294, 'precision_1': 0.8702684488374706, 'recall_0': 0.8791461755781103, 'recall_1': 0.8922155688622755}

Classification Report for Best XGBClassifier:
              precision    recall  f1-score   support

           0       0.90      0.88      0.89      9557
           1       0.87      0.89      0.88      8684

    accuracy                           0.89     18241
   macro avg       0.89      0.89      0.89     18241
weighted avg       0.89      0.89      0.89     18241



##############################
LGBMClassifier Full Results
##############################
    n_estimators  max_depth  learning_rate  is_unbalance  f1_score  accuracy  precision_0  precision_1  recall_0  recall_1
0            100          5           0.01          True  0.552755  0.639110     0.621825     0.674068  0.794182  0.468448
1            100          5           0.01         False  0.477774  0.626446     0.598833     0.714253  0.869520  0.358936
2            100          5           0.10          True  0.789149  0.797599     0.811471     0.782801  0.799414  0.795601
3            100          5           0.10         False  0.769249  0.788060     0.779766     0.798513  0.829863  0.742054
4            100          5           0.20          True  0.832178  0.836796     0.858154     0.815130  0.824840  0.849954
5            100          5           0.20         False  0.827570  0.836522     0.841346     0.831127  0.847860  0.824044
6            100         10           0.01          True  0.632515  0.671619     0.667859     0.676864  0.742492  0.593620
7            100         10           0.01         False  0.569620  0.660764     0.634255     0.719178  0.832688  0.471557
8            100         10           0.10          True  0.810634  0.816074     0.836772     0.794974  0.806215  0.826923
9            100         10           0.10         False  0.804273  0.817170     0.814687     0.820108  0.842733  0.789037
10           100         10           0.20          True  0.850593  0.854229     0.877848     0.830572  0.838443  0.871603
11           100         10           0.20         False  0.843696  0.850885     0.858973     0.842051  0.855917  0.845348
12           100         15           0.01          True  0.633645  0.671564     0.668653     0.675577  0.739667  0.596614
13           100         15           0.01         False  0.572100  0.660819     0.635168     0.716190  0.828503  0.476278
14           100         15           0.10          True  0.812153  0.817992     0.837099     0.798331  0.810296  0.826462
15           100         15           0.10         False  0.802811  0.815416     0.814213     0.816828  0.839175  0.789268
16           100         15           0.20          True  0.847101  0.850940     0.873988     0.827783  0.836036  0.867342
17           100         15           0.20         False  0.847997  0.854997     0.862873     0.846392  0.859893  0.849608
18           200          5           0.01          True  0.657119  0.686311     0.687311     0.685032  0.736214  0.631391
19           200          5           0.01         False  0.593547  0.674744     0.646982     0.732623  0.834572  0.498848
20           200          5           0.10          True  0.838997  0.843649     0.863967     0.822924  0.832688  0.855712
21           200          5           0.10         False  0.829064  0.838660     0.840647     0.836400  0.853929  0.821856
22           200          5           0.20          True  0.870364  0.873910     0.895132     0.852396  0.860103  0.889106
23           200          5           0.20         False  0.866998  0.872321     0.883896     0.859975  0.870671  0.874136
24           200         10           0.01          True  0.686304  0.703415     0.714227     0.691194  0.723344  0.681483
25           200         10           0.01         False  0.647749  0.697166     0.679356     0.725779  0.799205  0.584869
26           200         10           0.10          True  0.858170  0.861630     0.885201     0.838016  0.845558  0.879318
27           200         10           0.10         False  0.855646  0.862508     0.869019     0.855351  0.868473  0.855942
28           200         10           0.20          True  0.875993  0.878515     0.905356     0.852057  0.857801  0.901313
29           200         10           0.20         False  0.875535  0.880325     0.892819     0.867081  0.876844  0.884155
30           200         15           0.01          True  0.687931  0.704457     0.715855     0.691654  0.722821  0.684247
31           200         15           0.01         False  0.654599  0.701880     0.684196     0.729887  0.800460  0.593390
32           200         15           0.10          True  0.855727  0.859547     0.881532     0.837337  0.845558  0.874942
33           200         15           0.10         False  0.855074  0.861740     0.869369     0.853407  0.866276  0.856748
34           200         15           0.20          True  0.876766  0.879502     0.904830     0.854349  0.860521  0.900392
35           200         15           0.20         False  0.879418  0.883669     0.898574     0.868073  0.876949  0.891064
36           300          5           0.01          True  0.689332  0.709446     0.715763     0.702006  0.738830  0.677107
37           300          5           0.01         False  0.645665  0.701551     0.677883     0.742515  0.820027  0.571165
38           300          5           0.10          True  0.863442  0.867222     0.888236     0.845890  0.854034  0.881737
39           300          5           0.10         False  0.855222  0.861905     0.869410     0.853701  0.866590  0.856748
40           300          5           0.20          True  0.884403  0.887287     0.910384     0.864096  0.870566  0.905689
41           300          5           0.20         False  0.880550  0.884820     0.899314     0.869624  0.878518  0.891755
42           300         10           0.01          True  0.714515  0.727208     0.741232     0.711983  0.736424  0.717066
43           300         10           0.01         False  0.689308  0.725673     0.710417     0.747912  0.804227  0.639222
44           300         10           0.10          True  0.873137  0.875884     0.901641     0.850360  0.856545  0.897167
45           300         10           0.10         False  0.872393  0.877583     0.888500     0.865910  0.876321  0.878973
46           300         10           0.20          True  0.885815  0.888657     0.911797     0.865429  0.871822  0.907186
47           300         10           0.20         False  0.883121  0.887177     0.902436     0.871246  0.879774  0.895325
48           300         15           0.01          True  0.715667  0.727482     0.742851     0.710990  0.733912  0.720405
49           300         15           0.01         False  0.692470  0.728195     0.712858     0.750471  0.805797  0.642791
50           300         15           0.10          True  0.876870  0.879557     0.905197     0.854133  0.860207  0.900852
51           300         15           0.10         False  0.872686  0.877474     0.890715     0.863488  0.873287  0.882082
52           300         15           0.20          True  0.885470  0.888603     0.909724     0.867189  0.874124  0.904537
53           300         15           0.20         False  0.886994  0.891015     0.905497     0.875842  0.884273  0.898434

Best Parameters for LGBMClassifier:
{'n_estimators': 300, 'max_depth': 15, 'learning_rate': 0.2, 'is_unbalance': False, 'f1_score': 0.886994088221919, 'accuracy': 0.8910147469985198, 'precision_0': 0.90549662487946, 'precision_1': 0.8758419398293669, 'recall_0': 0.8842733075232814, 'recall_1': 0.8984339014279134}

Classification Report for Best LGBMClassifier:
              precision    recall  f1-score   support

           0       0.91      0.88      0.89      9557
           1       0.88      0.90      0.89      8684

    accuracy                           0.89     18241
   macro avg       0.89      0.89      0.89     18241
weighted avg       0.89      0.89      0.89     18241



##############################
Ensemble Model Performance
##############################
Ensemble Accuracy: 0.8889
Ensemble Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.90      0.89      9557
           1       0.88      0.88      0.88      8684

    accuracy                           0.89     18241
   macro avg       0.89      0.89      0.89     18241
weighted avg       0.89      0.89      0.89     18241

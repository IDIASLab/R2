{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxIZErHduZIK",
        "outputId": "9e152467-4adb-4111-9dc5-db81548239cd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_curve,auc,classification_report,make_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzGIB1p89ytc"
      },
      "outputs": [],
      "source": [
        "from link_prediction import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvGS35YXno_Y"
      },
      "outputs": [],
      "source": [
        "#Load the entity embeddings dict\n",
        "with open(\"fb15k237_transe_entity_embeddings.pkl\", \"rb\") as f:\n",
        "    entity_embeddings = pickle.load(f)\n",
        "#Load the predicate embeddings dict\n",
        "with open(\"fb15k237_transe_predicate_embeddings.pkl\", \"rb\") as f:\n",
        "    predicate_embeddings = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa2LFenqu9wS"
      },
      "outputs": [],
      "source": [
        "train_triples = pd.read_csv('fb15k237_train.txt', dtype=str, sep='\\t', header=None, names=['head', 'relation', 'tail'])\n",
        "test_triples = pd.read_csv('fb15k237_test.txt', dtype=str, sep='\\t', header=None, names=['head', 'relation', 'tail'])\n",
        "#valid_triples = pd.read_csv('fb15k237_valid.txt', dtype=str, sep='\\t', header=None, names=['head', 'relation', 'tail'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRqnxNxEEp7D",
        "outputId": "b4af2e04-f63f-4b34-95f8-01392e5e44eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Precomputing top-k predictions: 100%|██████████| 20466/20466 [33:19<00:00, 10.24it/s]\n"
          ]
        }
      ],
      "source": [
        "tripleEvaluator=TripleEvaluator(entity_embeddings,predicate_embeddings,train_triples,test_triples, model='TransE', k=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGZBN8-4Etl3"
      },
      "outputs": [],
      "source": [
        "#Create or acquire the training data (negative samples)\n",
        "#tail_df = tripleEvaluator.create_training_data_filtered(n=1, creating_for=\"tail\")\n",
        "#tail_df.to_csv('/content/drive/MyDrive/link_prediction/nguyen_fb15k237_transe_tail_df_1_ns.csv', index=False)\n",
        "tail_df = pd.read_csv('nguyen_fb15k237_transe_tail_df_1_ns.csv')\n",
        "#head_df = tripleEvaluator.create_training_data_filtered(n=1, creating_for=\"head\")\n",
        "#head_df.to_csv('/content/drive/MyDrive/link_prediction/nguyen_fb15k237_transe_head_df_1_ns.csv', index=False)\n",
        "head_df = pd.read_csv('nguyen_fb15k237_transe_head_df_1_ns.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_9FIvAFFCNy"
      },
      "outputs": [],
      "source": [
        "train_triples['label']=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z8Nz2WLwF0u"
      },
      "outputs": [],
      "source": [
        "train_df = pd.concat([head_df,tail_df,train_triples], axis=0)\n",
        "# Reset the index\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "# Drop duplicate rows\n",
        "train_df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "WzTNqTK4FQ4j",
        "outputId": "3d1b0bc1-0c62-4c6b-ada9-5888c1214fe7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>272115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>146168</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "label\n",
              "1    272115\n",
              "0    146168\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4RXmYKNtxAh"
      },
      "outputs": [],
      "source": [
        "# Step 1: Prepare the training data\n",
        "train_df['embedding'] = train_df.apply(lambda row: tripleEvaluator.get_embedding(row), axis=1)\n",
        "X_train = np.vstack(train_df['embedding'].values)  # Stack embeddings into a matrix\n",
        "y_train = train_df['label'].values  # Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gh7qdLaDHVA"
      },
      "outputs": [],
      "source": [
        "# Step 1: Split the data into training and test sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    np.vstack(train_df['embedding'].values),  # Embeddings matrix\n",
        "    train_df['label'].values,                # Labels\n",
        "    test_size=0.1,                           # 10% of the data for testing\n",
        "    random_state=42,                         # For reproducibility\n",
        "    stratify=train_df['label'].values        # Maintain label distribution\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTrGMFhwWfzb",
        "outputId": "e64bc1a7-e12b-4a47-9598-fa444ff04df2"
      },
      "outputs": [],
      "source": [
        "# Step 1: Initialize the MLP model\n",
        "mlp_model = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 128),  # Two layers with k units each\n",
        "    activation='relu',              # ReLU activation function\n",
        "    solver='adam',                  # Adam optimizer\n",
        "    alpha=0.0001,                   # L2 regularization\n",
        "    learning_rate_init=0.001,       # Learning rate\n",
        "    early_stopping=True,            # Stops if validation score does not improve\n",
        "    max_iter=25,                     # Number of iterations\n",
        "    batch_size=32,                  # Same batch size\n",
        "    random_state=42                 # For reproducibility\n",
        ")\n",
        "mlp_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Evaluate the model on the test set\n",
        "y_pred = mlp_model.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxAxuk58wyP0",
        "outputId": "713c13ad-7b55-44e4-9ef7-f302c9632189"
      },
      "outputs": [],
      "source": [
        "# Create a LightGBM model \n",
        "lgbm_model = LGBMClassifier(\n",
        "    #class_weight={0: 2, 1: 1}, # To handle imbalanced data\n",
        "    n_estimators=100,        # Number of boosting rounds\n",
        "    max_depth=15,            # Maximum depth of a tree\n",
        "    #num_leaves= 100,              # Maximum number of leaves in one tree\n",
        "    #min_child_samples=25 ,       # Minimum data per leaf\n",
        "    #subsample= 1,\n",
        "    random_state=42          # Set a random state for reproducibility\n",
        ")\n",
        "# Fit the model\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lgbm = lgbm_model.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"LightGBM Classification Report:\\n\", classification_report(y_test, y_pred_lgbm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gCuAPuA0HK_",
        "outputId": "83e02818-1ceb-4b45-cf98-6925f06e0c41"
      },
      "outputs": [],
      "source": [
        "# Create an XGBoost model \n",
        "xgb_model = XGBClassifier(\n",
        "    #scale_pos_weight=0.5,    # Adjust the weight for the positive class (useful for imbalance)\n",
        "    n_estimators=100,       # Number of boosting rounds\n",
        "    max_depth=10,           # Maximum depth of a tree\n",
        "    #min_child_weight=,\n",
        "    #subsample=,\n",
        "    #colsample_bytree=\n",
        "    random_state=42         # Set a random state for reproducibility\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd9W3fwtZVjd",
        "outputId": "f5d4bfae-9c31-4168-b423-5c906e920dab"
      },
      "outputs": [],
      "source": [
        "# Create the voting classifier\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('lgbm', lgbm_model),\n",
        "        ('mlp', mlp_model)\n",
        "    ],\n",
        "    voting='soft',  # Soft voting to use probabilities\n",
        "    n_jobs=-1       # Use all cores for parallel processing\n",
        ")\n",
        "\n",
        "# Fit the ensemble model (if needed)\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the ensemble\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "\n",
        "print(\"Ensemble Accuracy:\", accuracy_score(y_test, y_pred_ensemble))\n",
        "print(\"Classification Report for Ensemble:\\n\", classification_report(y_test, y_pred_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb2mCtwHudxX",
        "outputId": "b77c3db1-bd2d-4ef5-9a23-c1fea008e7f1"
      },
      "outputs": [],
      "source": [
        "# Create the voting classifier\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('lgbm', lgbm_model),\n",
        "        ('mlp', mlp_model)\n",
        "    ],\n",
        "    voting='soft',  # Soft voting to use probabilities\n",
        "    n_jobs=-1       # Use all cores for parallel processing\n",
        ")\n",
        "\n",
        "# Fit the ensemble model (if needed)\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the ensemble\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "\n",
        "print(\"Ensemble Accuracy:\", accuracy_score(y_test, y_pred_ensemble))\n",
        "print(\"Classification Report for Ensemble:\\n\", classification_report(y_test, y_pred_ensemble))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZSKDVJ6ZDNh"
      },
      "source": [
        "**ORIGINIAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q3MjGjuiCgv",
        "outputId": "7dfa5b3c-ff80-4a61-a93b-6593b302d638"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "# Evaluate for k from 1 to 10 and store the results in a dictionary\n",
        "for k in [1, 3, 5, 10, 20, 30, 40, 50, 100]:\n",
        "    total_count_head, hit_count_head = tripleEvaluator.evaluate_hitk_original_2_on_patterns(k=k, evaluate_for=\"head\")\n",
        "    total_count_tail, hit_count_tail = tripleEvaluator.evaluate_hitk_original_2_on_patterns(k=k, evaluate_for=\"tail\")\n",
        "\n",
        "    overall_hit_count = hit_count_head + hit_count_tail\n",
        "    overall_total_count = total_count_head + total_count_tail\n",
        "    overall_percentage = (overall_hit_count / overall_total_count) * 100\n",
        "\n",
        "    results[k] = {\n",
        "        \"head\": {\n",
        "            \"total_count\": len(tripleEvaluator.precomputed_top_k_head),\n",
        "            \"hit_count\": hit_count_head,\n",
        "            \"percentage\": hit_count_head / total_count_head * 100\n",
        "        },\n",
        "        \"tail\": {\n",
        "            \"total_count\": len(tripleEvaluator.precomputed_top_k_tail),\n",
        "            \"hit_count\": hit_count_tail,\n",
        "            \"percentage\": hit_count_tail / total_count_tail * 100\n",
        "        },\n",
        "        \"overall\": {\n",
        "            \"hit_count\": overall_hit_count,\n",
        "            \"total_count\": overall_total_count,\n",
        "            \"percentage\": overall_percentage\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Print the results nicely\n",
        "for k, data in results.items():\n",
        "    print(\n",
        "        f\"k={k}: \"\n",
        "        f\"Head: {data['head']['percentage']:.2f}% | \"\n",
        "        f\"Tail: {data['tail']['percentage']:.2f}% | \"\n",
        "        f\"Overall: {data['overall']['percentage']:.2f}%\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uILILUuZNl-"
      },
      "source": [
        "**MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGr4ioF-R07-",
        "outputId": "87053bb8-0002-468e-d90e-b8b64638491d"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# Define the mapping of k values to their respective threshold lists\n",
        "k_threshold_mapping = {\n",
        "    1: [1, 3, 5, 10, 20, 30, 40, 50],\n",
        "    3: [3, 5, 10, 20, 30, 40, 50],\n",
        "    5: [5, 10, 20, 30, 40, 50],\n",
        "    10: [10, 20, 30, 40, 50]\n",
        "}\n",
        "\n",
        "# Iterate through the mapping\n",
        "for k, threshold_values in k_threshold_mapping.items():\n",
        "    for threshold in threshold_values:\n",
        "        # Evaluate for head\n",
        "        total_count_head, hit_count_head = tripleEvaluator.rerank(k=k, model=mlp_model, threshold=threshold, evaluate_for=\"head\")\n",
        "        head_percentage = hit_count_head / total_count_head * 100\n",
        "\n",
        "        # Evaluate for tail\n",
        "        total_count_tail, hit_count_tail = tripleEvaluator.rerank(k=k, model=mlp_model, threshold=threshold, evaluate_for=\"tail\")\n",
        "        tail_percentage = hit_count_tail / total_count_tail * 100\n",
        "\n",
        "        # Calculate overall metrics\n",
        "        overall_hit_count = hit_count_head + hit_count_tail\n",
        "        overall_total_count = total_count_head + total_count_tail\n",
        "        overall_percentage = (overall_hit_count / overall_total_count) * 100\n",
        "\n",
        "        # Store the results in a dictionary\n",
        "        results.append({\n",
        "            \"k\": k,\n",
        "            \"threshold\": threshold,\n",
        "            \"head\": f\"{head_percentage:.2f}%\",\n",
        "            \"tail\": f\"{tail_percentage:.2f}%\",\n",
        "            \"overall\": f\"{overall_percentage:.2f}%\"\n",
        "        })\n",
        "\n",
        "# Print the results in a formatted way\n",
        "for result in results:\n",
        "    print(\n",
        "        f\"k={result['k']} | th={result['threshold']} | \"\n",
        "        f\"head: {result['head']} | tail: {result['tail']} | overall: {result['overall']}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGwVzcm7ZSSU"
      },
      "source": [
        "**XGB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFSJ40k4Y-TB",
        "outputId": "6d204825-73b7-4e56-f4e0-ea7bcfbf0774"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# Define the mapping of k values to their respective threshold lists\n",
        "k_threshold_mapping = {\n",
        "    1: [1, 3, 5, 10, 20, 30, 40, 50],\n",
        "    3: [3, 5, 10, 20, 30, 40, 50],\n",
        "    5: [5, 10, 20, 30, 40, 50],\n",
        "    10: [10, 20, 30, 40, 50]\n",
        "}\n",
        "\n",
        "# Iterate through the mapping\n",
        "for k, threshold_values in k_threshold_mapping.items():\n",
        "    for threshold in threshold_values:\n",
        "        # Evaluate for head\n",
        "        total_count_head, hit_count_head = tripleEvaluator.rerank(k=k, model=xgb_model, threshold=threshold, evaluate_for=\"head\")\n",
        "        head_percentage = hit_count_head / total_count_head * 100\n",
        "\n",
        "        # Evaluate for tail\n",
        "        total_count_tail, hit_count_tail = tripleEvaluator.rerank(k=k, model=xgb_model, threshold=threshold, evaluate_for=\"tail\")\n",
        "        tail_percentage = hit_count_tail / total_count_tail * 100\n",
        "\n",
        "        # Calculate overall metrics\n",
        "        overall_hit_count = hit_count_head + hit_count_tail\n",
        "        overall_total_count = total_count_head + total_count_tail\n",
        "        overall_percentage = (overall_hit_count / overall_total_count) * 100\n",
        "\n",
        "        # Store the results in a dictionary\n",
        "        results.append({\n",
        "            \"k\": k,\n",
        "            \"threshold\": threshold,\n",
        "            \"head\": f\"{head_percentage:.2f}%\",\n",
        "            \"tail\": f\"{tail_percentage:.2f}%\",\n",
        "            \"overall\": f\"{overall_percentage:.2f}%\"\n",
        "        })\n",
        "\n",
        "# Print the results in a formatted way\n",
        "for result in results:\n",
        "    print(\n",
        "        f\"k={result['k']} | th={result['threshold']} | \"\n",
        "        f\"head: {result['head']} | tail: {result['tail']} | overall: {result['overall']}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNOH_58QZWkf"
      },
      "source": [
        "**LGBM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebl-O7y0Zc_g",
        "outputId": "e4aef2d9-7037-4526-9586-929556d310d2"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# Define the mapping of k values to their respective threshold lists\n",
        "k_threshold_mapping = {\n",
        "    1: [1, 3, 5, 10, 20, 30, 40, 50],\n",
        "    3: [3, 5, 10, 20, 30, 40, 50],\n",
        "    5: [5, 10, 20, 30, 40, 50],\n",
        "    10: [10, 20, 30, 40, 50]\n",
        "}\n",
        "\n",
        "# Iterate through the mapping\n",
        "for k, threshold_values in k_threshold_mapping.items():\n",
        "    for threshold in threshold_values:\n",
        "        # Evaluate for head\n",
        "        total_count_head, hit_count_head = tripleEvaluator.rerank(k=k, model=lgbm_model, threshold=threshold, evaluate_for=\"head\")\n",
        "        head_percentage = hit_count_head / total_count_head * 100\n",
        "\n",
        "        # Evaluate for tail\n",
        "        total_count_tail, hit_count_tail = tripleEvaluator.rerank(k=k, model=lgbm_model, threshold=threshold, evaluate_for=\"tail\")\n",
        "        tail_percentage = hit_count_tail / total_count_tail * 100\n",
        "\n",
        "        # Calculate overall metrics\n",
        "        overall_hit_count = hit_count_head + hit_count_tail\n",
        "        overall_total_count = total_count_head + total_count_tail\n",
        "        overall_percentage = (overall_hit_count / overall_total_count) * 100\n",
        "\n",
        "        # Store the results in a dictionary\n",
        "        results.append({\n",
        "            \"k\": k,\n",
        "            \"threshold\": threshold,\n",
        "            \"head\": f\"{head_percentage:.2f}%\",\n",
        "            \"tail\": f\"{tail_percentage:.2f}%\",\n",
        "            \"overall\": f\"{overall_percentage:.2f}%\"\n",
        "        })\n",
        "\n",
        "# Print the results in a formatted way\n",
        "for result in results:\n",
        "    print(\n",
        "        f\"k={result['k']} | th={result['threshold']} | \"\n",
        "        f\"head: {result['head']} | tail: {result['tail']} | overall: {result['overall']}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctr0UjH9Z6D9"
      },
      "source": [
        "**SCKIT LEARN ENSEMBLE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vbaqPDJaAQA",
        "outputId": "1f7b3beb-51be-404c-dc0c-4fd16200ebcb"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# Define the mapping of k values to their respective threshold lists\n",
        "k_threshold_mapping = {\n",
        "    1: [1, 3, 5, 10, 20, 30, 40, 50],\n",
        "    3: [3, 5, 10, 20, 30, 40, 50],\n",
        "    5: [5, 10, 20, 30, 40, 50],\n",
        "    10: [10, 20, 30, 40, 50]\n",
        "}\n",
        "\n",
        "# Iterate through the mapping\n",
        "for k, threshold_values in k_threshold_mapping.items():\n",
        "    for threshold in threshold_values:\n",
        "        # Evaluate for head\n",
        "        total_count_head, hit_count_head = tripleEvaluator.rerank(k=k, model=ensemble_model, threshold=threshold, evaluate_for=\"head\")\n",
        "        head_percentage = hit_count_head / total_count_head * 100\n",
        "\n",
        "        # Evaluate for tail\n",
        "        total_count_tail, hit_count_tail = tripleEvaluator.rerank(k=k, model=ensemble_model, threshold=threshold, evaluate_for=\"tail\")\n",
        "        tail_percentage = hit_count_tail / total_count_tail * 100\n",
        "\n",
        "        # Calculate overall metrics\n",
        "        overall_hit_count = hit_count_head + hit_count_tail\n",
        "        overall_total_count = total_count_head + total_count_tail\n",
        "        overall_percentage = (overall_hit_count / overall_total_count) * 100\n",
        "\n",
        "        # Store the results in a dictionary\n",
        "        results.append({\n",
        "            \"k\": k,\n",
        "            \"threshold\": threshold,\n",
        "            \"head\": f\"{head_percentage:.2f}%\",\n",
        "            \"tail\": f\"{tail_percentage:.2f}%\",\n",
        "            \"overall\": f\"{overall_percentage:.2f}%\"\n",
        "        })\n",
        "\n",
        "# Print the results in a formatted way\n",
        "for result in results:\n",
        "    print(\n",
        "        f\"k={result['k']} | th={result['threshold']} | \"\n",
        "        f\"head: {result['head']} | tail: {result['tail']} | overall: {result['overall']}\"\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
